<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Tracing the Representation Geometry of Language Models from Pretraining to Post-training - Li et al.">
  <meta name="description" content="We uncover a consistent non-monotonic sequence of three geometric phases during LLM pretraining through spectral analysis of representation geometry using RankMe and Î±-ReQ metrics.">
  <meta name="keywords" content="language models, representation geometry, pretraining, post-training, spectral analysis, RankMe, eigenspectrum, OLMo, Pythia, machine learning, deep learning">
  <meta name="author" content="Melody Zixuan Li, Kumar Krishna Agrawal, Arna Ghosh, Komal Kumar Teru, Adam Santoro, Guillaume Lajoie, Blake A. Richards">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Mila - Quebec AI Institute & McGill University">
  <meta property="og:title" content="Tracing the Representation Geometry of Language Models from Pretraining to Post-training">
  <meta property="og:description" content="We uncover a consistent non-monotonic sequence of three geometric phases during LLM pretraining through spectral analysis of representation geometry using RankMe and Î±-ReQ metrics.">
  <meta property="og:url" content="https://melodylizx.github.io/llm-geometry-project/">
  <meta property="og:image" content="https://melodylizx.github.io/llm-geometry-project/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Tracing the Representation Geometry of Language Models - Research Preview">
  <meta property="article:published_time" content="2025-09-27T00:00:00.000Z">
  <meta property="article:author" content="Melody Zixuan Li">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="language models">
  <meta property="article:tag" content="representation geometry">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <meta name="twitter:creator" content="@YOUR_TWITTER_HANDLE">
  <meta name="twitter:title" content="Tracing the Representation Geometry of Language Models from Pretraining to Post-training">
  <meta name="twitter:description" content="We uncover a consistent non-monotonic sequence of three geometric phases during LLM pretraining through spectral analysis of representation geometry using RankMe and Î±-ReQ metrics.">
  <meta name="twitter:image" content="https://melodylizx.github.io/llm-geometry-project/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="Tracing the Representation Geometry of Language Models - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Tracing the Representation Geometry of Language Models from Pretraining to Post-training">
  <meta name="citation_author" content="Li, Melody Zixuan">
  <meta name="citation_author" content="Agrawal, Kumar Krishna">
  <meta name="citation_author" content="Ghosh, Arna">
  <meta name="citation_author" content="Teru, Komal Kumar">
  <meta name="citation_author" content="Santoro, Adam">
  <meta name="citation_author" content="Lajoie, Guillaume">
  <meta name="citation_author" content="Richards, Blake A.">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="NeurIPS 2025">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2509.23024.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>Tracing the Representation Geometry of Language Models from Pretraining to Post-training - Li et al. | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Tracing the Representation Geometry of Language Models from Pretraining to Post-training",
    "description": "Standard training metrics like loss fail to explain the emergence of complex capabilities in large language models. We take a spectral approach to investigate the geometry of learned representations across pretraining and post-training, measuring effective rank (RankMe) and eigenspectrum decay (Î±-ReQ).",
    "author": [
      {
        "@type": "Person",
        "name": "Melody Zixuan Li",
        "affiliation": {
          "@type": "Organization",
          "name": "McGill University & Mila - Quebec AI Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Kumar Krishna Agrawal",
        "affiliation": {
          "@type": "Organization",
          "name": "UC Berkeley"
        }
      },
      {
        "@type": "Person",
        "name": "Arna Ghosh",
        "affiliation": {
          "@type": "Organization",
          "name": "McGill University & Mila - Quebec AI Institute"
        }
      },
      {
        "@type": "Person",
        "name": "Komal Kumar Teru",
        "affiliation": {
          "@type": "Organization",
          "name": "Cohere"
        }
      },
      {
        "@type": "Person",
        "name": "Adam Santoro",
        "affiliation": {
          "@type": "Organization",
          "name": "Google Deepmind"
        }
      },
      {
        "@type": "Person",
        "name": "Guillaume Lajoie",
        "affiliation": {
          "@type": "Organization",
          "name": "Mila - Quebec AI Institute & UniversitÃ© de MontrÃ©al"
        }
      },
      {
        "@type": "Person",
        "name": "Blake A. Richards",
        "affiliation": {
          "@type": "Organization",
          "name": "Mila - Quebec AI Institute & McGill University"
        }
      }
    ],
    "datePublished": "2025-09-27",
    "publisher": {
      "@type": "Organization",
      "name": "NeurIPS 2025"
    },
    "url": "https://melodylizx.github.io/llm-geometry-project/",
    "image": "https://melodylizx.github.io/llm-geometry-project/static/images/social_preview.png",
    "keywords": ["language models", "representation geometry", "pretraining", "spectral analysis", "RankMe", "eigenspectrum"],
    "abstract": "Standard training metrics like loss fail to explain the emergence of complex capabilities in large language models. We take a spectral approach to investigate the geometry of learned representations across pretraining and post-training, measuring effective rank (RankMe) and eigenspectrum decay (Î±-ReQ). With OLMo (1B-7B) and Pythia (160M-12B) models, we uncover a consistent non-monotonic sequence of three geometric phases during autoregressive pretraining.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "about": [
      {
        "@type": "Thing",
        "name": "Language Models"
      },
      {
        "@type": "Thing", 
        "name": "Representation Learning"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Mila - Quebec AI Institute",
    "url": "https://mila.quebec",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico"
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- Add your lab's related works here -->
        <a href="#" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Related Paper 1</h5>
            <p>Brief description of the related work from your lab.</p>
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="#" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Related Paper 2</h5>
            <p>Brief description of another related work.</p>
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Tracing the Representation Geometry of Language Models from Pretraining to Post-training</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://melodylizx.github.io/" target="_blank">Melody Zixuan Li</a><sup>1,2,*</sup>,</span>
              <span class="author-block">
                <a href="https://people.eecs.berkeley.edu/~krishna/" target="_blank">Kumar Krishna Agrawal</a><sup>3,*</sup>,</span>
              <span class="author-block">
                <a href="https://arnaghosh.github.io/" target="_blank">Arna Ghosh</a><sup>1,2,9,*</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=ZkxLTT4AAAAJ&hl=en" target="_blank">Komal Kumar Teru</a><sup>4</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=evIkDWoAAAAJ&hl=en" target="_blank">Adam Santoro</a><sup>5,â€ </sup>,</span>
              <span class="author-block">
                <a href="https://guillaumelajoie.com/" target="_blank">Guillaume Lajoie</a><sup>2,6,9</sup>,</span>
              <span class="author-block">
                <a href="https://sites.google.com/mila.quebec/linc-lab/team/blake?authuser=0" target="_blank">Blake A. Richards</a><sup>1,2,7,8,9</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Computer Science, McGill University</span>
              <span class="author-block"><sup>2</sup>Mila - Quebec AI Institute</span>
              <span class="author-block"><sup>3</sup>UC Berkeley</span>
              <span class="author-block"><sup>4</sup>Cohere</span>
              <span class="author-block"><sup>5</sup>Google Deepmind</span><br>
              <span class="author-block"><sup>6</sup>Mathematics and Statistics, UniversitÃ© de MontrÃ©al</span>
              <span class="author-block"><sup>7</sup>Neurology & Neurosurgery and Montreal Neurological Institute, McGill University</span><br>
              <span class="author-block"><sup>8</sup>CIFAR Learning in Machines & Brains Program</span>
              <span class="author-block"><sup>9</sup>Google, Paradigms of Intelligence Team</span><br>
              <span class="eql-cntrb"><small><sup>*</sup>Equal contribution</small></span>
              <span class="eql-cntrb"><small><sup>â€ </sup>Advisory capacity only</small></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">NeurIPS 2025</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2509.23024.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Code - update with your GitHub repository URL when available -->
              <span class="link-block">
                <a href="https://github.com/YOUR_REPO_HERE" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code (Coming Soon)</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://arxiv.org/abs/2509.23024" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="ai ai-arxiv"></i>
              </span>
              <span>arXiv</span>
            </a>
          </span>
        </div>
      </div>
    </div>
  </div>
</div>
</div>
</section>


<!-- Teaser video/image section -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Replace with your teaser figure/video -->
      <img src="static/images/figure0.jpg" alt="Three geometric phases of LLM pretraining" style="width:100%">
      <h2 class="subtitle has-text-centered">
        We discover three distinct geometric phases during language model pretraining: warmup (representational collapse), entropy-seeking (dimensionality expansion with peak n-gram memorization), and compression-seeking (anisotropic consolidation leading to improved downstream performance).
      </h2>
    </div>
  </div>
</section>
<!-- End teaser -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Standard training metrics like loss fail to explain the emergence of complex capabilities in large language models. We take a spectral approach to investigate the geometry of learned representations across pretraining and post-training, measuring effective rank (RankMe) and eigenspectrum decay (Î±-ReQ). 
          </p>
          <p>
            With OLMo (1B-7B) and Pythia (160M-12B) models, we uncover a consistent non-monotonic sequence of three geometric phases during autoregressive pretraining. The initial "warmup" phase exhibits rapid representational collapse. This is followed by an "entropy-seeking" phase, where the manifold's dimensionality expands substantially, coinciding with peak n-gram memorization. Subsequently, a "compression-seeking" phase imposes anisotropic consolidation, selectively preserving variance along dominant eigendirections while contracting others, a transition marked with significant improvement in downstream task performance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Method Overview -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            We primarily consider the final layer, last token activations as the representations for a given input sequence and study the geometry of the corresponding manifold. To analyze the intrinsic geometry of the manifold, we compute the feature covariance matrix.
          </p>
          <p>
            We analyze the representation geometry of language models during training using two complementary spectral metrics:
          </p>
          <ul>
            <li><strong>RankMe (Effective Rank):</strong> Measures the effective dimensionality of the representation manifold</li>
            <li><strong>Î±-ReQ (Eigenspectrum Decay):</strong> Quantifies how quickly eigenvalues decay, indicating anisotropic structure</li>
          </ul>
          <p>
            We apply these metrics to study OLMo (1B-7B parameters) and Pythia (160M-12B parameters) models across their full pretraining trajectories and post-training phases.
          </p>
        </div>
      </div>
    </div>
    <!-- Method Figure 1 -->
    <div class="columns is-centered has-text-centered" style="margin-top: 30px;">
      <div class="column is-four-fifths">
        <img src="static/images/figure1.jpeg" alt="Method visualization" style="max-width:100%; margin: 20px auto;">
        <p class="subtitle"><strong>Figure 1:</strong> Spectral framework reveals three universal phases in LLM training. (A) LLM representations analyzed via empirical feature covariance Î£Ì‚(f<sub>Î¸</sub>) of last-token hidden states f<sub>Î¸</sub>(x<sub>i</sub>). (B) Two complementary spectral metrics: Î±-ReQ measures eigenspectrum decay rate (variance concentration), while RankMe quantifies effective rank (utilized dimensionality).</p>
      </div>
    </div>
  </div>
</section>

<!-- Key Findings Section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Key Findings</h2>
      
      <!-- Finding 1: Three Geometric Phases -->
      <div class="content" style="margin-bottom: 50px;">
        <h3 class="title is-4">ðŸ”„ Three Geometric Phases</h3>
        <div class="content has-text-justified">
          <p>
            We find that while the pretraining loss decreases monotonically, the spectral changes are non-monotonic! In particular, we find that LLMs undergo 3 distinct phases during pretraining:
          </p>
          <ul>
            <li><strong>Warmup:</strong> Rapid compression which collapses representation to dominant directions</li>
            <li><strong>Entropy-seeking:</strong> Manifold expansion, adding information in non-dominant directions</li>
            <li><strong>Compression-seeking:</strong> Anisotropic consolidation, selectively packing more information in dominant directions</li>
          </ul>
        </div>
        <div class="content has-text-centered" style="margin-top: 30px;">
          <img src="static/images/figure2.jpeg" alt="Three geometric phases" style="max-width:90%; margin: 20px auto;">
          <p class="subtitle"><strong>Figure 2:</strong> Non-monotonic spectral changes across three distinct geometric phases during pretraining.</p>
        </div>
      </div>

      <!-- Finding 2: Memorization vs Generalization -->
      <div class="content" style="margin-bottom: 50px;">
        <h3 class="title is-4">ðŸ“Š Memorization vs Generalization Across Phases</h3>
        <div class="content has-text-justified">
          <p>
            Does the representation complexity inform us about changes in LLM behavior? To understand this better, we investigate LLM behavior from the lens of memorization vs generalization across the different phases.
          </p>
          <p>
            Strikingly, we find that the model uses characteristically different mechanisms as it optimizes the next-token prediction objective:
          </p>
          <ul>
            <li><strong>Entropy-seeking phase:</strong> Correlates with short-sequence memorization, as measured using n-gram alignment</li>
            <li><strong>Compression-seeking phase:</strong> Correlates with dramatic gains in factual reasoning requiring long-range dependencies (e.g., TriviaQA)</li>
          </ul>
        </div>
        <div class="content has-text-centered" style="margin-top: 30px;">
          <img src="static/images/figure3.jpeg" alt="Memorization vs generalization" style="max-width:90%; margin: 20px auto;">
          <p class="subtitle"><strong>Figure 3:</strong> Different phases correlate with distinct behavioral patterns: memorization in entropy-seeking phase and generalization in compression-seeking phase.</p>
        </div>
      </div>

      <!-- Finding 3: Post-training -->
      <div class="content" style="margin-bottom: 50px;">
        <h3 class="title is-4">ðŸŽ¯ Post-training: SFT/DPO vs RLVR</h3>
        <div class="content has-text-justified">
          <p>
            Post training: we find key differences between SFT/DPO and RLVR:
          </p>
          <ul>
            <li><strong>SFT & DPO</strong> exhibit entropy-seeking expansion, favoring instruction memorization but reducing OOD robustness</li>
            <li><strong>RLVR</strong> exhibits compression-seeking consolidation, learning reward-aligned behaviors at the cost of reduced exploration</li>
          </ul>
          <p>
            We believe this rank-consolidation helps explain why base models can recover better performance at high pass@K compared to RLVR-tuned models.
          </p>
        </div>
        <div class="content has-text-centered" style="margin-top: 30px;">
          <img src="static/images/figure4.jpeg" alt="Post-training comparison" style="max-width:90%; margin: 20px auto;">
          <p class="subtitle"><strong>Figure 4:</strong> Contrasting geometric changes in SFT/DPO (entropy-seeking) versus RLVR (compression-seeking) post-training.</p>
        </div>
      </div>

      <!-- Finding 4: Why do these phases arise? -->
      <div class="content" style="margin-bottom: 50px;">
        <h3 class="title is-4">ðŸ”¬ Why Do These Geometric Phases Arise?</h3>
        <div class="content has-text-justified">
          <p>
            We show, both analytically and with simulations in a toy model, that gradient descent dynamics with cross-entropy loss, coupled with skewed token frequencies and representation bottlenecks, are the key reasons underlying these non-monotonic spectral changes.
          </p>
        </div>
        <div class="content has-text-centered" style="margin-top: 30px;">
          <img src="static/images/figure5.gif" alt="Toy model simulation" style="max-width:90%; margin: 20px auto;">
          <p class="subtitle"><strong>Figure 5:</strong> Toy model simulations showing how gradient descent dynamics lead to non-monotonic spectral changes.</p>
        </div>
      </div>

      <!-- Finding 5: Task-relevant information in eigendirections -->
      <div class="content" style="margin-bottom: 50px;">
        <h3 class="title is-4">ðŸ’¡ Task-Relevant Information in Spectral Tail</h3>
        <div class="content has-text-justified">
          <p>
            Is task-relevant info contained in the top eigendirections? To understand this we project the activations to a top-K / bottom-K subspaces and measure performance on standard benchmarks.
          </p>
          <p>
            Surprisingly, we find that in line with our theoretical results, the spectral tail encodes critical task-relevant information, while the dominant directions are expendable for many tasks!
          </p>
          <p>
            In particular, on SciQ:
          </p>
          <ul>
            <li>Removing top 50 directions barely hurts accuracy</li>
            <li>Retaining only top 50 directions collapses performance</li>
          </ul>
        </div>
        <div class="content has-text-centered" style="margin-top: 30px;">
          <img src="static/images/figure6.png" alt="Eigendirection analysis" style="max-width:90%; margin: 20px auto;">
          <p class="subtitle"><strong>Figure 6:</strong> Task-relevant information is concentrated in the spectral tail rather than dominant eigendirections.</p>
        </div>
      </div>

    </div>
  </div>
</section>

<!-- Results - Individual Figures -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Additional Results</h2>

<!-- Video presentation (if available) -->
<!-- Uncomment and update when you have a video
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/YOUR_VIDEO_ID" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
-->

<!-- Poster (if available) -->
<!-- Uncomment and update when you have a poster
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>
      <iframe src="static/pdfs/poster.pdf" width="100%" height="550"></iframe>
    </div>
  </div>
</section>
-->

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="bibtex-header">
      <h2 class="title">BibTeX</h2>
      <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
        <i class="fas fa-copy"></i>
        <span class="copy-text">Copy</span>
      </button>
    </div>
    <pre id="bibtex-code"><code>@inproceedings{li2025tracing,
  title={Tracing the Representation Geometry of Language Models from Pretraining to Post-training},
  author={Li, Melody Zixuan and Agrawal, Kumar Krishna and Ghosh, Arna and Teru, Komal Kumar and Santoro, Adam and Lajoie, Guillaume and Richards, Blake A.},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2025},
  url={https://arxiv.org/abs/2509.23024}
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
<!-- End of Statcounter Code -->

</body>
</html>
